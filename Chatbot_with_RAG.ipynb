{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuel3347/Chatbot-with-RAG/blob/main/Chatbot_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading important libraries\n"
      ],
      "metadata": {
        "id": "-XpZ5UnbZu51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNNoWN15Hb_8"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    langchain==0.0.354 \\\n",
        "    openai==1.6.1 \\\n",
        "    datasets==2.10.1 \\\n",
        "    pinecone-client==3.0.0 \\\n",
        "    tiktoken==0.5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Chatbot"
      ],
      "metadata": {
        "id": "H3PB7So7Z7Kz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv7M6IocHhT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721c5bd6-009c-4943-f2ea-5df4d3068fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \"YOUR_API_KEY\"\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNofdWJXHxRz"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "SystemMessage,\n",
        "HumanMessage,\n",
        "AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant,\"),\n",
        "    HumanMessage(content=\"Hello AI, how are you today\"),\n",
        "    AIMessage(content=\"I'm great thank you,how can I help you?\"),\n",
        "    HumanMessage(content=\"I'd like to understand moore's laws.\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL24HL5wH3Bp"
      },
      "outputs": [],
      "source": [
        "res = chat(messages)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIBbqzvBH5pW",
        "outputId": "3eda1f58-3a1b-4cc3-f6b8-1929462b2278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moore's Law is a principle in the field of computing that was formulated by Gordon Moore, co-founder of Intel Corporation, in 1965. The law states that the number of transistors on a microchip doubles approximately every two years, leading to a continuous increase in computing power and performance while decreasing the cost of manufacturing. This trend has held true for several decades, driving rapid advancements in technology such as faster processors, increased memory capacity, and smaller devices. Moore's Law has been a driving force behind the exponential growth of the digital age and continues to shape the development of modern computing devices.\n"
          ]
        }
      ],
      "source": [
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the chunked data"
      ],
      "metadata": {
        "id": "TRQMee_HaAeK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuQdf0yVH-rf",
        "outputId": "a480dc48-c5f0-438c-ac48-7e0f3421a7d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/jamescalam___json/jamescalam--ai-arxiv-chunked-c0ecde7e34f06e42/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
              "    num_rows: 41584\n",
              "})"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"jamescalam/ai-arxiv-chunked\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6seBsfidIDM9",
        "outputId": "acccce47-089e-4176-9c65-217b758d051c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'doi': '1910.01108',\n",
              " 'chunk-id': '0',\n",
              " 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n",
              " 'id': '1910.01108',\n",
              " 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
              " 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.',\n",
              " 'source': 'http://arxiv.org/pdf/1910.01108',\n",
              " 'authors': ['Victor Sanh',\n",
              "  'Lysandre Debut',\n",
              "  'Julien Chaumond',\n",
              "  'Thomas Wolf'],\n",
              " 'categories': ['cs.CL'],\n",
              " 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\\n  - NeurIPS 2019',\n",
              " 'journal_ref': None,\n",
              " 'primary_category': 'cs.CL',\n",
              " 'published': '20191002',\n",
              " 'updated': '20200301',\n",
              " 'references': [{'id': '1910.01108'}]}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dpP_g8W6ZrqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk0_HocdIaVM"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "api_key  = os.getenv(\"PINECONE_API_KEY\") or \"YOUR_API_KEY\"\n",
        "pc = Pinecone(api_key = api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_b2y-kQItIE"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upserting into Vectors"
      ],
      "metadata": {
        "id": "2zrHeK-8aJhi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7QwRwgmIiX-"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "index_name = 'ai-rag'\n",
        "existing_indexes = [\n",
        "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
        "]\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in existing_indexes:\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of ada 002\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-9grOqqJH1f"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhDQ5QGRJIzu"
      },
      "outputs": [],
      "source": [
        "texts = [\n",
        "    'this is the first chunk of text',\n",
        "    'then another second chunk of text is here'\n",
        "]\n",
        "\n",
        "res = embed_model.embed_documents(texts)\n",
        "len(res), len(res[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keug59YjJMtT"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm  # for progress bar\n",
        "\n",
        "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    # get batch of data\n",
        "    batch = data.iloc[i:i_end]\n",
        "    # generate unique ids for each chunk\n",
        "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
        "    # get text to embed\n",
        "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
        "    # embed text\n",
        "    embeds = embed_model.embed_documents(texts)\n",
        "    # get metadata to store in Pinecone\n",
        "    metadata = [\n",
        "        {'text': x['chunk'],\n",
        "         'source': x['source'],\n",
        "         'title': x['title']} for i, x in batch.iterrows()\n",
        "    ]\n",
        "    # add to Pinecone\n",
        "    index.upsert(vectors=zip(ids, embeds, metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uizBSaHuJRZa"
      },
      "outputs": [],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retreival Augmented Generation"
      ],
      "metadata": {
        "id": "aJBRBA_RaYpK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8A_TP1oSXAQ",
        "outputId": "0d52c2ab-9c45-49f7-89af-51528a99e44e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"  # the metadata field that contains our text\n",
        "\n",
        "# initialize the vector store object\n",
        "vectorstore = Pinecone(\n",
        "    index, embed_model.embed_query, text_field\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTQe0CI1SfL8",
        "outputId": "b32f1f27-7815-49a7-9bd2-d823b2dcb1e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='CodeNet: A Large-Scale AI for Code Dataset for\\nLearning a Diversity of Coding Tasks\\nRuchir Puri1, David S. Kung1, Geert Janssen1, Wei Zhang1,\\nGiacomo Domeniconi1,Vladimir Zolotov1,Julian Dolby1,Jie Chen2,1,\\nMihir Choudhury1,Lindsey Decker1,Veronika Thost2,1,Luca Buratti1,\\nSaurabh Pujar1,Shyam Ramji1,Ulrich Finkler1,Susan Malaika3,Frederick Reiss1\\n1IBM Research\\n2MIT-IBM Watson AI Lab\\n3IBM Worldwide Ecosystems\\nAbstract\\nOver the last several decades, software has been woven into the fabric of every\\naspect of our society. As software development surges and code infrastructure of\\nenterprise applications ages, it is now more critical than ever to increase software\\ndevelopment productivity and modernize legacy applications. Advances in deep\\nlearning and machine learning algorithms have enabled breakthroughs in computer\\nvision, speech recognition, natural language processing and beyond, motivating\\nresearchers to leverage AI techniques to improve software development efﬁciency.\\nThus, the fast-emerging research area of “AI for Code” has garnered new interest', metadata={'source': 'http://arxiv.org/pdf/2105.12655', 'title': 'CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks'}),\n",
              " Document(page_content='has\\nthe\\nopportunity\\nto\\nwrite\\nhigh-quality\\npapers\\nthat\\ncould\\nbe\\naccepted\\nat\\npremier\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nresearch\\nconferences”\\n(Schwartz\\net\\nal.,\\n2019).\\nIf\\nAI\\nis\\nmore\\ncompute-efficient\\nto\\nthe\\npoint\\nwhere\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nit\\nrequires\\nonly\\na\\nlaptop\\nor\\nother\\nrelatively\\nobtainable\\nhardware,\\nthe\\nfield\\nof\\nAI\\nmay\\nbecome\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmuch\\nmore\\naccessible.\\nCompute-efficient\\nmachine\\nlearning\\ncould\\nthereby\\nhave\\na\\nsizable\\nsocial\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nimpact.\\n \\n \\nThe\\nsecond\\nand\\nthird\\npillars,\\nfederated\\nlearning\\nand\\ndata\\nsovereignty,\\ndirectly\\ntarget\\nAI', metadata={'source': 'http://arxiv.org/pdf/2006.06217', 'title': 'SECure: A Social and Environmental Certificate for AI Systems'}),\n",
              " Document(page_content='Goldberg. 2021. Formalizing trust in artiﬁcial intelligence: Prerequisites, causes and goals of human\\ntrust in ai. Proc. FAccT .Heesoo Jang. 2021. A South Korean chatbot shows just\\nhow sloppy tech companies can be with user data.\\nJoel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha,\\nMoontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022. Knowledge unlearning for mitigating privacy risks in language models. ArXiv ,\\nabs/2210.01504.\\nGanesh Jawahar, Muhammad Abdul-Mageed, and\\nLaks Lakshmanan, V .S. 2020. Automatic detection\\nof machine generated text: A critical survey. In\\nProceedings of the 28th International Conference\\non Computational Linguistics , pages 2296–2309,\\nBarcelona, Spain (Online). International Committee\\non Computational Linguistics.\\nHaozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan\\nZhu, and Minlie Huang. 2020. Language generation\\nwith multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP) , pages 725–736, Online. Association', metadata={'source': 'http://arxiv.org/pdf/2210.07700', 'title': 'Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey'})]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What is so special about AI?\"\n",
        "\n",
        "vectorstore.similarity_search(query, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjwY7RssSkaV"
      },
      "outputs": [],
      "source": [
        "def augment_prompt(query: str):\n",
        "  results = vectorstore.similarity_search(query, k=3)\n",
        "  source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "  augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "  Contexts:\n",
        "  {source_knowledge}\n",
        "\n",
        "  Query: {query}\"\"\"\n",
        "  return augmented_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajaOGsLSSqqP",
        "outputId": "cfd901fb-c859-476b-dbd2-319f13f9b064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI is special because it has the potential to significantly impact society by improving software development efficiency, making AI more accessible with compute-efficient machine learning, and addressing issues such as data sovereignty and trust in AI.\n"
          ]
        }
      ],
      "source": [
        "# create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(query)\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fNLI9bhS7YU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a796d9-ffb5-483f-afec-da025ec2ec21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Safety measures used to develop AI include:\n",
            "\n",
            "1. Data Privacy Protection: Ensuring that sensitive data used in AI systems is protected and only accessed by authorized personnel.\n",
            "\n",
            "2. Bias Detection and Mitigation: Identifying and addressing biases in AI algorithms to prevent discriminatory outcomes.\n",
            "\n",
            "3. Robustness Testing: Conducting rigorous testing to ensure that AI systems function as intended and can handle unexpected scenarios.\n",
            "\n",
            "4. Explainability and Transparency: Making AI systems transparent and understandable so that their decisions can be explained to users and stakeholders.\n",
            "\n",
            "5. Ethical Guidelines: Adhering to ethical principles and guidelines in the development and deployment of AI systems to ensure they are used responsibly.\n",
            "\n",
            "These safety measures help mitigate risks associated with AI technology and promote the development of safe and reliable AI systems.\n"
          ]
        }
      ],
      "source": [
        "prompt = HumanMessage(\n",
        "    content=\"what safety measures are used to develop AI?\"\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deleting the indexes to save our resources"
      ],
      "metadata": {
        "id": "epOe7cQkajO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pc.delete_index(index_name)"
      ],
      "metadata": {
        "id": "sl24ZRIfmKWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdbI74EDFh1GxpPEc3I44Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}